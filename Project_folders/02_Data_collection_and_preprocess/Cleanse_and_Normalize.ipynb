{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanse and Normalize the Data, addressing issues like duplicates, missing values, and inconsistent formatting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP models cannot directly process raw PDF files, which are designed for visual presentation, not data extraction\n",
    "\n",
    " 1. _Extraction:_ Getting the text out of the PDFs using libraries like `PyPDF2`, `pdfminer.six`, or `OCR` if needed.\n",
    " 2. _Text Cleaning:_ Removing noise like headers, footers, special characters, and standardizing the text.\n",
    " 3. _Organization (Optional but Recommended):_ Structuring the extracted text into sentences, paragraphs, or sections, and potentially extracting metadata.\n",
    "\n",
    "Preprocessing PDF data is a crucial step for achieving good results with NLP models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a breakdown of the steps with code examples using popular libraries. Keep in mind that the specific steps and code will vary depending on the complexity and structure of your PDF files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. _Extraction:_\n",
    "  - Install necessary libraries:\n",
    "    ```\n",
    "    pip install PyPDF2 pdfminer.six tabula-py\n",
    "    # For OCR if needed:\n",
    "    pip install pytesseract Pillow\n",
    "    # You might also need to install the Tesseract OCR engine separately\n",
    "    # depending on your operating system.\n",
    "\n",
    "  - Extract text from PDFs (without tables):\n",
    "    ```\n",
    "    import PyPDF2\n",
    "    def extract_text_from_pdf(pdf_path):\n",
    "        text = \"\"\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                page = reader.pages[page_num]\n",
    "                text += page.extract_text()\n",
    "        return text\n",
    "\n",
    "    pdf_file = \"your_document.pdf\"\n",
    "    raw_text = extract_text_from_pdf(pdf_file)\n",
    "    print(raw_text[:500]) # Print first 500 characters to inspect\n",
    "\n",
    "  - Extract text from PDFs (with potential for better layout handling):\n",
    "    ```\n",
    "    from pdfminer.high_level import extract_text\n",
    "\n",
    "    def extract_text_with_pdfminer(pdf_path):\n",
    "        text = extract_text(pdf_path)\n",
    "        return text\n",
    "\n",
    "    pdf_file = \"your_document.pdf\"\n",
    "    raw_text = extract_text_with_pdfminer(pdf_file)\n",
    "    print(raw_text[:500])\n",
    "\n",
    "  - Extract text from scanned PDFs (using OCR):\n",
    "    ```\n",
    "    from PIL import Image\n",
    "    import pytesseract\n",
    "    import PyPDF2\n",
    "\n",
    "    def extract_text_from_scanned_pdf(pdf_path):\n",
    "        text = \"\"\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                page = reader.pages[page_num]\n",
    "                # Save each page as an image\n",
    "                image = page.to_image()\n",
    "                image.save(f\"temp_page_{page_num}.png\")\n",
    "                # Use Tesseract to extract text from the image\n",
    "                page_text = pytesseract.image_to_string(Image.open(f\"temp_page_{page_num}.png\"))\n",
    "                text += page_text\n",
    "                import os\n",
    "                os.remove(f\"temp_page_{page_num}.png\") # Clean up temporary image\n",
    "        return text\n",
    "\n",
    "    pdf_file = \"scanned_document.pdf\"\n",
    "    raw_text = extract_text_from_scanned_pdf(pdf_file)\n",
    "    print(raw_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. _Text Cleaning:_\n",
    "  - Install NLTK (for more advanced text processing):\n",
    "    ```\n",
    "    pip install nltk\n",
    "    import nltk\n",
    "    nltk.download('punkt') # Download sentence tokenizer\n",
    "    nltk.download('stopwords') # Download stop words list\n",
    "  - Basic Cleaning Functions:\n",
    "    ```\n",
    "    import re\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "\n",
    "    def clean_text(text):\n",
    "        # Remove headers, footers, page numbers (adjust regex based on your document structure)\n",
    "        text = re.sub(r'^\\s*[\\d]+\\s*$', '', text, flags=re.MULTILINE) # Remove lines with only numbers (likely page numbers)\n",
    "        text = re.sub(r'^\\s*.*?header.*?\\n', '', text, flags=re.MULTILINE | re.IGNORECASE) # Example header removal\n",
    "        text = re.sub(r'\\n.*?footer.*?\\s*$', '', text, flags=re.MULTILINE | re.IGNORECASE) # Example footer removal\n",
    "\n",
    "        # Handle line breaks and hyphenation\n",
    "        text = re.sub(r'-\\n', '', text) # Join hyphenated words split across lines\n",
    "        text = re.sub(r'\\n', ' ', text) # Replace newlines with spaces\n",
    "\n",
    "        # Remove special characters and punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        return text\n",
    "\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    print(cleaned_text[:500])\n",
    "\n",
    "  - More Advanced Cleaning (using NLTK):\n",
    "    ```\n",
    "    def advanced_clean_text(text):\n",
    "        text = clean_text(text) # Apply basic cleaning first\n",
    "\n",
    "        # Tokenization\n",
    "        sentences = sent_tokenize(text)\n",
    "        words = [word for sent in sentences for word in sent.split()]\n",
    "\n",
    "        # Remove stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "\n",
    "        # Consider stemming or lemmatization (NLTK provides tools for this)\n",
    "        # from nltk.stem import PorterStemmer\n",
    "        # stemmer = PorterStemmer()\n",
    "        # words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "        return \" \".join(words)\n",
    "\n",
    "    advanced_cleaned_text = advanced_clean_text(raw_text)\n",
    "    print(advanced_cleaned_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. _Organization:_\n",
    "  - Sentence Segmentation (already done in advanced_clean_text):\n",
    "    ```\n",
    "    sentences = sent_tokenize(cleaned_text)\n",
    "    print(sentences[:5])\n",
    "\n",
    "  - Paragraph Segmentation (can be tricky, often relies on blank lines):\n",
    "    ```\n",
    "    def get_paragraphs(text):\n",
    "        # Split by double newlines (common paragraph separator)\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        # Clean up each paragraph\n",
    "        paragraphs = [p.strip() for p in paragraphs if p.strip()]\n",
    "        return paragraphs\n",
    "\n",
    "    paragraphs = get_paragraphs(raw_text) # Use raw_text as cleaning might remove paragraph breaks\n",
    "    print(paragraphs[:2])\n",
    "\n",
    "  - Document Structuring (more complex, depends heavily on PDF structure):\n",
    "    - This often requires more specific logic based on visual cues or consistent formatting patterns in your PDFs. You might need to identify headings, subheadings, and associate text with them. Libraries like `pdfminer.six` can sometimes provide more information about the layout that can be helpful here.\n",
    "\n",
    "  - Metadata Extraction (if available in the PDF):\n",
    "    ```\n",
    "    def extract_metadata(pdf_path):\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            info = reader.metadata\n",
    "        return info\n",
    "\n",
    "    pdf_file = \"your_document.pdf\"\n",
    "    metadata = extract_metadata(pdf_file)\n",
    "    print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together (Example Workflow):\n",
    "```\n",
    "pdf_file = \"your_document.pdf\"\n",
    "\n",
    "# 1. Extract Text\n",
    "raw_text = extract_text_with_pdfminer(pdf_file)\n",
    "\n",
    "# 2. Clean Text\n",
    "cleaned_text = clean_text(raw_text)\n",
    "# or\n",
    "advanced_cleaned_text = advanced_clean_text(raw_text)\n",
    "\n",
    "# 3. Organize (Example: Sentence Segmentation)\n",
    "sentences = sent_tokenize(cleaned_text)\n",
    "print(sentences[:10])\n",
    "\n",
    "# If you have tables:\n",
    "tables = extract_tables_from_pdf(pdf_file)\n",
    "for i, table in enumerate(tables):\n",
    "    # Process each table (e.g., convert to string, clean)\n",
    "    table_string = table.to_string()\n",
    "    cleaned_table_string = clean_text(table_string)\n",
    "    print(f\"--- Cleaned Table {i+1} ---\")\n",
    "    print(cleaned_table_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important Considerations:\n",
    " - _Complexity of PDFs:_ The more complex the layout of your PDFs (e.g., multiple columns, images with text overlays), the more challenging extraction and cleaning will be.\n",
    " - _Consistency:_ If your PDFs have a consistent structure, you can write more targeted cleaning and organization logic.\n",
    " - _Error Handling:_ Implement error handling (e.g., try-except blocks) to gracefully manage issues like corrupted PDFs or unexpected formatting.\n",
    " - _Inspection:_ Always inspect the output of each step to ensure the cleaning and organization are working as expected. Print snippets of the text or tables to verify.\n",
    " - _Iterative Process:_ Cleaning and organizing data is often an iterative process. You might need to adjust your cleaning functions based on the initial results.\n",
    " - _Task-Specific Cleaning:_ The level of cleaning required depends on your specific NLP task. For example, you might not need to remove stop words for named entity recognition.\n",
    "By following these steps and adapting the code to your specific PDF data, you can effectively clean and organize your data for NLP tasks in Python. Remember to start with simpler methods and gradually increase complexity as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To iterate through all PDFs in a directory, we need to modify the code to:\n",
    "1. Get a list of all PDF files in the directory.\n",
    "2. Loop through that list, applying the extraction, cleaning, and organization functions to each PDF file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import os\n",
    "import PyPDF2\n",
    "from pdfminer.high_level import extract_text\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        text = \"\"\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                page = reader.pages[page_num]\n",
    "                text += page.extract_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path} with PyPDF2: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_with_pdfminer(pdf_path):\n",
    "    try:\n",
    "        text = extract_text(pdf_path)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdf_path} with pdfminer.six: {e}\")\n",
    "        return None\n",
    "\n",
    "def clean_text(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    # Remove headers, footers, page numbers (adjust regex based on your document structure)\n",
    "    text = re.sub(r'^\\s*[\\d]+\\s*$', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'^\\s*.*?header.*?\\n', '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    text = re.sub(r'\\n.*?footer.*?\\s*$', '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
    "    text = re.sub(r'-\\n', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def process_pdf(pdf_path):\n",
    "    print(f\"Processing: {pdf_path}\")\n",
    "    # 1. Extract Text (try different methods)\n",
    "    raw_text = extract_text_with_pdfminer(pdf_path)\n",
    "    if not raw_text:\n",
    "        raw_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    if raw_text:\n",
    "        # 2. Clean Text\n",
    "        cleaned_text = clean_text(raw_text)\n",
    "\n",
    "        # 3. Organize (Example: Sentence Segmentation)\n",
    "        sentences = sent_tokenize(cleaned_text)\n",
    "        print(f\"First 10 sentences from {pdf_path}:\\n{sentences[:10]}\\n{'='*50}\")\n",
    "        # You can do further processing here, like storing the data\n",
    "\n",
    "    else:\n",
    "        print(f\"Could not extract text from {pdf_path}\")\n",
    "\n",
    "# Specify the directory containing your PDF files\n",
    "pdf_directory = \"/path/to/your/pdf/directory\"\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "all_files = os.listdir(pdf_directory)\n",
    "\n",
    "# Filter the list to include only PDF files (you might need to adjust the extension check)\n",
    "pdf_files = [os.path.join(pdf_directory, f) for f in all_files if f.lower().endswith(\".pdf\")]\n",
    "\n",
    "# Iterate through the list of PDF files and process each one\n",
    "for pdf_file_path in pdf_files:\n",
    "    process_pdf(pdf_file_path)\n",
    "\n",
    "print(\"Finished processing all PDF files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Explanation of Changes:_\n",
    " - _import os:_ This line imports the os module, which provides functions for interacting with the operating system, including listing files in a directory.\n",
    " - pdf_directory = \"/path/to/your/pdf/directory\": You need to replace this placeholder with the actual path to the directory where your PDF files are located.\n",
    " - *os.listdir(pdf_directory):* This gets a list of all files and directories within the specified directory.\n",
    " - *[os.path.join(pdf_directory, f) for f in all_files if f.lower().endswith(\".pdf\")]:* This uses a list comprehension to:\n",
    "   - Iterate through each item (f) in the all_files list.\n",
    "   - Check if the lowercase version of the filename ends with \".pdf\".\n",
    "   - If it does, it constructs the full path to the file using os.path.join() and adds it to the pdf_files list.\n",
    " - *process_pdf(pdf_path) function:* This function encapsulates the steps of extracting, cleaning, and organizing the text for a single PDF file. This makes the main loop cleaner.\n",
    " - *for pdf_file_path in pdf_files::* This loop iterates through the pdf_files list, and for each PDF file path, it calls the process_pdf() function.\n",
    " - *Error Handling:* I've added basic try-except blocks within the extraction functions to handle potential errors that might occur when processing certain PDF files. This prevents the entire script from crashing if one file has issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to Use:\n",
    " - Replace \"/path/to/your/pdf/directory\" with the correct path to your PDF folder.\n",
    " - Make sure you have the necessary libraries installed (PyPDF2, pdfminer.six, nltk).\n",
    " - Run the Python script.\n",
    " \n",
    "This script will now iterate through all the PDF files in the specified directory, process each one, and print the first 10 sentences of the cleaned text for each PDF. You can then adapt the process_pdf() function to do whatever further analysis or storage you need with the processed data.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
