{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert model v 01\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "from datetime import datetime\n",
    "from PyPDF2.generic import IndirectObject\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Collection and Preparation:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helping functions to extract metadata from PDF files\n",
    "def resolve_metadata_value(value):\n",
    "    if isinstance(value, IndirectObject):\n",
    "        value = value.get_object()\n",
    "    return str(value) if value is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helping function\n",
    "def extract_pdf_info(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text\n",
    "\n",
    "        metadata = reader.metadata\n",
    "        if metadata is not None:\n",
    "            title = resolve_metadata_value(metadata.get('/Title')) or 'Unknown Title'\n",
    "            author = resolve_metadata_value(metadata.get('/Author')) or 'Unknown Author'\n",
    "            creation_date = resolve_metadata_value(metadata.get('/CreationDate')) or ''\n",
    "            mod_date = resolve_metadata_value(metadata.get('/ModDate')) or ''\n",
    "        else:\n",
    "            title = 'Unknown Title'\n",
    "            author = 'Unknown Author'\n",
    "            creation_date = ''\n",
    "            mod_date = ''\n",
    "\n",
    "        # Extract date from metadata\n",
    "        date_str = creation_date or mod_date\n",
    "        date = None\n",
    "        if date_str:\n",
    "            date_match = re.search(r\"D:(\\d{14})\", date_str)\n",
    "            if date_match:\n",
    "                try:\n",
    "                    date = datetime.strptime(date_match.group(1), '%Y%m%d%H%M%S')\n",
    "                except ValueError:\n",
    "                    pass  # Handle invalid date format gracefully\n",
    "\n",
    "        return {'title': title, 'author': author, 'date': date, 'text': text}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create a dictionary from the Public database, with keys: title, author, date, text, and filename*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing data as PDFs\n",
    "pdf_dir = '../data/AWS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadError(\"Invalid Elementary Object starting with b'\\\\\\\\' @508629: b'r (pdfTeX-1.40.21)\\\\n \\\\\\\\par /Author()/Title()/Subject()/Creator(LaTeX with hyperref'\")\n"
     ]
    }
   ],
   "source": [
    "pdf_info_list = []\n",
    "\n",
    "for pdf_file in os.listdir(pdf_dir):\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "        pdf_info = extract_pdf_info(pdf_path)\n",
    "        pdf_info['filename'] = pdf_file\n",
    "        pdf_info_list.append(pdf_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Second, Create a dictionary from the podcasts and book, with keys: title, author, date, text, and filename*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing additional PDFs\n",
    "pdf_dir = '../data/Training_docs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aditional list with proven docs\n",
    "pdf_proven_list = []\n",
    "\n",
    "for pdf_file in os.listdir(pdf_dir):\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "        pdf_info = extract_pdf_info(pdf_path)\n",
    "        pdf_info['filename'] = pdf_file\n",
    "        pdf_proven_list.append(pdf_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Finally, create a dictionary from the sections of the book, with keys: title, author, date, text, and filename*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing additional PDFs\n",
    "pdf_dir = '../data/Training_docs/tokenized_sections'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aditional list with proven docs\n",
    "pdf_book_sections_list = []\n",
    "\n",
    "for pdf_file in os.listdir(pdf_dir):\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "        pdf_info = extract_pdf_info(pdf_path)\n",
    "        pdf_info['filename'] = pdf_file\n",
    "        pdf_book_sections_list.append(pdf_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine with existing pdf_info_list\n",
    "combined_data = pdf_info_list + pdf_proven_list + pdf_book_sections_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*let's save them for future use if needed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the combined_data to a JSON file\n",
    "# Define a serialization function for datetime objects\n",
    "def serialize_datetime(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "# Assuming pdf_info_list is your list of dictionaries\n",
    "with open('../data/combined_data.json', 'w') as json_file:\n",
    "    json.dump(combined_data, json_file, default=serialize_datetime, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pdf_info_list to a JSON file\n",
    "# Define a serialization function for datetime objects\n",
    "def serialize_datetime(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "# Assuming pdf_info_list is your list of dictionaries\n",
    "with open('../data/pdf_info_list.json', 'w') as json_file:\n",
    "    json.dump(pdf_info_list, json_file, default=serialize_datetime, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Assign labels and save only the combined data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your list of healthspan-related keywords\n",
    "healthspan_keywords = ['longevity', 'aging', 'senescence', 'lifespan', 'healthspan', 'caloric restriction', 'telomere', 'autophagy', 'gerontology', 'anti-aging', 'resveratrol', 'sirtuins', 'mTOR', 'NAD+', 'oxidative stress', 'inflammation', 'mitochondria', 'genomics', 'epigenetics', 'stem cells', 'regeneration', 'DNA repair', 'protein folding', 'calorie restriction', 'intermittent fasting', 'blue zones', 'hormesis', 'geroprotector', 'rapamycin', 'metformin', 'amyloids', 'proteostasis', 'senolytics', 'leptin', 'circadian rhythm', 'sleep', 'exercise', 'diet', 'nutrition', 'microbiome', 'gut health', 'probiotics', 'prebiotics', 'polyphenols', 'flavonoids', 'antioxidants','hormone replacement', 'testosterone', 'stress management', 'mindfulness', 'meditation', 'cognitive function', 'neuroplasticity', 'brain health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "healthspan_keywords.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_document(text, keywords, threshold=3):\n",
    " \"\"\"Assign a label based on the presence of at least 3 healthspan-related keywords.\"\"\"\n",
    " text_lower = text.lower()\n",
    " count = sum(1 for keyword in keywords if keyword in text_lower)\n",
    " return 1 if count >= threshold else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply labeling to each document\n",
    "for pdf in combined_data:\n",
    " pdf['label'] = label_document(pdf['text'], healthspan_keywords)\n",
    "\n",
    "# renundantly, unneded but it may be useful\n",
    "for pdf in pdf_info_list:\n",
    "    pdf['label'] = label_document(pdf['text'], healthspan_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the labeled data to a new JSON file\n",
    "# Define a serialization function for datetime objects\n",
    "def serialize_datetime(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "# this is our list of dictionaries\n",
    "with open('../data/combined_data.json', 'w') as json_file:\n",
    "    json.dump(combined_data, json_file, default=serialize_datetime, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2. Text preprocessing.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\email\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\email\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    return ' '.join(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\email\\desktop\\ds_science_youth_extension\\2504_science_youth\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\email\\desktop\\ds_science_youth_extension\\2504_science_youth\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 709/709 [52:18<00:00,  4.43s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to each document with a progress bar\n",
    "for pdf in tqdm(pdf_info_list, desc=\"Processing PDFs\"):\n",
    "    pdf['processed_text'] = preprocess_text(pdf['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|          | 0/740 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 740/740 [59:00<00:00,  4.78s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to each document with a progress bar\n",
    "for pdf in tqdm(combined_data, desc=\"Processing PDFs\"):\n",
    "    pdf['processed_text'] = preprocess_text(pdf['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "with open('../data/preprocessed_combined_data.json', 'w') as json_file:\n",
    "    json.dump(combined_data, json_file, default=serialize_datetime, indent=4)\n",
    "\n",
    "with open('../data/preprocessed_pdf_info_list.json', 'w') as json_file:\n",
    "    json.dump(pdf_info_list, json_file, default=serialize_datetime, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "740"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3. Split dataset between train and eval\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the dataset from the JSON file\n",
    "with open('../data/preprocessed_combined_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract texts and labels\n",
    "texts = [entry['processed_text'] for entry in data]\n",
    "labels = [entry['label'] for entry in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data: 80% for training, 20% for evaluation\n",
    "texts_train, texts_eval, labels_train, labels_eval = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "4. Model Selection and Training\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with BioBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    get_scheduler,\n",
    "    EarlyStoppingCallback\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HealthspanDataset(Dataset):\n",
    "   def __init__(self, texts, labels, tokenizer, max_len, default_text=\"default\"):\n",
    "        # Ensure all elements in texts are strings; convert or replace with default_text if not\n",
    "        self.texts = [str(text) if isinstance(text, (str, int, float)) else default_text for text in texts]\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "   def __len__(self):\n",
    "       return len(self.texts)\n",
    "\n",
    "   def __getitem__(self, idx):\n",
    "       text = self.texts[idx]\n",
    "       label = self.labels[idx]\n",
    "       encoding = self.tokenizer(\n",
    "           text,\n",
    "           max_length=self.max_len,\n",
    "           padding='max_length',\n",
    "           truncation=True,\n",
    "           return_tensors='pt',\n",
    "       )\n",
    "       return {\n",
    "           'input_ids': encoding['input_ids'].squeeze(),\n",
    "           'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "           'labels': torch.tensor(label, dtype=torch.long),\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define maximum sequence length\n",
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BioBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instances\n",
    "train_dataset = HealthspanDataset(texts_train, labels_train, tokenizer, max_len=MAX_LEN)\n",
    "eval_dataset = HealthspanDataset(texts_eval, labels_eval, tokenizer, max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "#texts = [pdf['processed_text'] for pdf in pdf_info_list]\n",
    "#labels = [pdf['label'] for pdf in pdf_info_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "#dataset = HealthspanDataset(texts, labels, tokenizer, max_len=512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize BioBERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\", num_labels=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\email\\Desktop\\DS_Science_Youth_Extension\\2504_Science_Youth\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "   output_dir='./results',\n",
    "   num_train_epochs=4,\n",
    "   per_device_train_batch_size=8,\n",
    "   per_device_eval_batch_size=8,\n",
    "   warmup_steps=500,\n",
    "   weight_decay=0.01,\n",
    "   logging_dir='./logs',\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   save_strategy=\"epoch\",  # Ensure this matches evaluation_strategy\n",
    "   load_best_model_at_end=True,\n",
    "   metric_for_best_model=\"eval_loss\",  # Or another metric you're monitoring\n",
    "   greater_is_better=False,  # Set to True if a higher metric is better\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "num_training_steps = len(train_dataset) * training_args.num_train_epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  # Ensure you have an evaluation dataset\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='296' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 14/296 09:50 < 3:51:17, 0.02 it/s, Epoch 0.18/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
