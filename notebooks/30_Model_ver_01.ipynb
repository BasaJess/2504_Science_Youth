{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert model v 01\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "from datetime import datetime\n",
    "from PyPDF2.generic import IndirectObject\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Collection and Preparation:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helping functions to extract metadata from PDF files\n",
    "def resolve_metadata_value(value):\n",
    "    if isinstance(value, IndirectObject):\n",
    "        value = value.get_object()\n",
    "    return str(value) if value is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helping function\n",
    "def extract_pdf_info(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text\n",
    "\n",
    "        metadata = reader.metadata\n",
    "        if metadata is not None:\n",
    "            title = resolve_metadata_value(metadata.get('/Title')) or 'Unknown Title'\n",
    "            author = resolve_metadata_value(metadata.get('/Author')) or 'Unknown Author'\n",
    "            creation_date = resolve_metadata_value(metadata.get('/CreationDate')) or ''\n",
    "            mod_date = resolve_metadata_value(metadata.get('/ModDate')) or ''\n",
    "        else:\n",
    "            title = 'Unknown Title'\n",
    "            author = 'Unknown Author'\n",
    "            creation_date = ''\n",
    "            mod_date = ''\n",
    "\n",
    "        # Extract date from metadata\n",
    "        date_str = creation_date or mod_date\n",
    "        date = None\n",
    "        if date_str:\n",
    "            date_match = re.search(r\"D:(\\d{14})\", date_str)\n",
    "            if date_match:\n",
    "                try:\n",
    "                    date = datetime.strptime(date_match.group(1), '%Y%m%d%H%M%S')\n",
    "                except ValueError:\n",
    "                    pass  # Handle invalid date format gracefully\n",
    "\n",
    "        return {'title': title, 'author': author, 'date': date, 'text': text}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create a dictionary from the Public database, with keys: title, author, date, text, and filename*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing data as PDFs\n",
    "pdf_dir = '../data/AWS/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadError(\"Invalid Elementary Object starting with b'\\\\\\\\' @508629: b'r (pdfTeX-1.40.21)\\\\n \\\\\\\\par /Author()/Title()/Subject()/Creator(LaTeX with hyperref'\")\n"
     ]
    }
   ],
   "source": [
    "pdf_info_list = []\n",
    "\n",
    "for pdf_file in os.listdir(pdf_dir):\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "        pdf_info = extract_pdf_info(pdf_path)\n",
    "        pdf_info['filename'] = pdf_file\n",
    "        pdf_info_list.append(pdf_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Second, Create a dictionary from the podcasts and book, with keys: title, author, date, text, and filename*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing additional PDFs\n",
    "pdf_dir = '../data/Training_docs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aditional list with proven docs\n",
    "pdf_proven_list = []\n",
    "\n",
    "for pdf_file in os.listdir(pdf_dir):\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "        pdf_info = extract_pdf_info(pdf_path)\n",
    "        pdf_info['filename'] = pdf_file\n",
    "        pdf_proven_list.append(pdf_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Finally, create a dictionary from the sections of the book, with keys: title, author, date, text, and filename*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing additional PDFs\n",
    "pdf_dir = '../data/Training_docs/tokenized_sections'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aditional list with proven docs\n",
    "pdf_book_sections_list = []\n",
    "\n",
    "for pdf_file in os.listdir(pdf_dir):\n",
    "    if pdf_file.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "        pdf_info = extract_pdf_info(pdf_path)\n",
    "        pdf_info['filename'] = pdf_file\n",
    "        pdf_book_sections_list.append(pdf_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine with existing pdf_info_list\n",
    "combined_data = pdf_info_list + pdf_proven_list + pdf_book_sections_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*let's save them for future use if needed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the combined_data to a JSON file\n",
    "# Define a serialization function for datetime objects\n",
    "def serialize_datetime(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "# Assuming pdf_info_list is your list of dictionaries\n",
    "with open('../data/combined_data.json', 'w') as json_file:\n",
    "    json.dump(combined_data, json_file, default=serialize_datetime, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pdf_info_list to a JSON file\n",
    "# Define a serialization function for datetime objects\n",
    "def serialize_datetime(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "# Assuming pdf_info_list is your list of dictionaries\n",
    "with open('../data/pdf_info_list.json', 'w') as json_file:\n",
    "    json.dump(pdf_info_list, json_file, default=serialize_datetime, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Assign labels and save only the combined data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your list of healthspan-related keywords\n",
    "healthspan_keywords = ['longevity', 'aging', 'senescence', 'lifespan', 'healthspan', 'caloric restriction', 'telomere', 'autophagy', 'gerontology', 'anti-aging', 'resveratrol', 'sirtuins', 'mTOR', 'NAD+', 'oxidative stress', 'inflammation', 'mitochondria', 'genomics', 'epigenetics', 'stem cells', 'regeneration', 'DNA repair', 'protein folding', 'calorie restriction', 'intermittent fasting', 'blue zones', 'hormesis', 'geroprotector', 'rapamycin', 'metformin', 'amyloids', 'proteostasis', 'senolytics', 'leptin', 'circadian rhythm', 'sleep', 'exercise', 'diet', 'nutrition', 'microbiome', 'gut health', 'probiotics', 'prebiotics', 'polyphenols', 'flavonoids', 'antioxidants','hormone replacement', 'testosterone', 'stress management', 'mindfulness', 'meditation', 'cognitive function', 'neuroplasticity', 'brain health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "healthspan_keywords.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_document(text, keywords, threshold=3):\n",
    " \"\"\"Assign a label based on the presence of at least 3 healthspan-related keywords.\"\"\"\n",
    " text_lower = text.lower()\n",
    " count = sum(1 for keyword in keywords if keyword in text_lower)\n",
    " return 1 if count >= threshold else 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply labeling to each document\n",
    "for pdf in combined_data:\n",
    " pdf['label'] = label_document(pdf['text'], healthspan_keywords)\n",
    "\n",
    "# renundantly, unneded but it may be useful\n",
    "for pdf in pdf_info_list:\n",
    "    pdf['label'] = label_document(pdf['text'], healthspan_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the labeled data to a new JSON file\n",
    "# Define a serialization function for datetime objects\n",
    "def serialize_datetime(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "# this is our list of dictionaries\n",
    "with open('../data/combined_data.json', 'w') as json_file:\n",
    "    json.dump(combined_data, json_file, default=serialize_datetime, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "2. Text preprocessing.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\email\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\email\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Tokenize\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    return ' '.join(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\email\\desktop\\ds_science_youth_extension\\2504_science_youth\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\email\\desktop\\ds_science_youth_extension\\2504_science_youth\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 709/709 [52:18<00:00,  4.43s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to each document with a progress bar\n",
    "for pdf in tqdm(pdf_info_list, desc=\"Processing PDFs\"):\n",
    "    pdf['processed_text'] = preprocess_text(pdf['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|          | 0/740 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  52%|█████▏    | 387/740 [29:35<44:00,  7.48s/it]  "
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to each document with a progress bar\n",
    "for pdf in tqdm(combined_data, desc=\"Processing PDFs\"):\n",
    "    pdf['processed_text'] = preprocess_text(pdf['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "with open('../data/preprocessed_combined_data.json', 'w') as json_file:\n",
    "    json.dump(combined_data, json_file, default=serialize_datetime, indent=4)\n",
    "\n",
    "with open('../data/preprocessed_pdf_info_list.json', 'w') as json_file:\n",
    "    json.dump(pdf_info_list, json_file, default=serialize_datetime, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "3. Model Selection and Training\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with BioBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AdamW,\n",
    "    get_scheduler,\n",
    "    EarlyStoppingCallback\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HealthspanDataset(Dataset):\n",
    "   def __init__(self, texts, labels, tokenizer, max_len):\n",
    "       self.texts = texts\n",
    "       self.labels = labels\n",
    "       self.tokenizer = tokenizer\n",
    "       self.max_len = max_len\n",
    "\n",
    "   def __len__(self):\n",
    "       return len(self.texts)\n",
    "\n",
    "   def __getitem__(self, idx):\n",
    "       text = self.texts[idx]\n",
    "       label = self.labels[idx]\n",
    "       encoding = self.tokenizer(\n",
    "           text,\n",
    "           max_length=self.max_len,\n",
    "           padding='max_length',\n",
    "           truncation=True,\n",
    "           return_tensors='pt',\n",
    "       )\n",
    "       return {\n",
    "           'input_ids': encoding['input_ids'].squeeze(),\n",
    "           'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "           'labels': torch.tensor(label, dtype=torch.long),\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "texts = [pdf['processed_text'] for pdf in pdf_info_list]\n",
    "labels = [pdf['label'] for pdf in pdf_info_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BioBERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = HealthspanDataset(texts, labels, tokenizer, max_len=512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BioBERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\", num_labels=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "   output_dir='./results',\n",
    "   num_train_epochs=4,\n",
    "   per_device_train_batch_size=8,\n",
    "   per_device_eval_batch_size=8,\n",
    "   warmup_steps=500,\n",
    "   weight_decay=0.01,\n",
    "   logging_dir='./logs',\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   load_best_model_at_end=True,\n",
    "   metric_for_best_model=\"eval_loss\",  # Or another metric you're monitoring\n",
    "   greater_is_better=False,  # Set to True if a higher metric is better\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "\n",
    "num_training_steps = len(dataset) * training_args.num_train_epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=eval_dataset,  # Ensure you have an evaluation dataset\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Train model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
